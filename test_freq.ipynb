{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the ability-index dict\n",
    "import json\n",
    "\n",
    "ability_index_dict = {}\n",
    "ability_json_file = \"src/ability_mindmap.json\"\n",
    "with open(ability_json_file,'r') as f:\n",
    "    ability_tree = json.load(f)\n",
    "\n",
    "# traverse the ability tree and assign index to each ability\n",
    "index = 0\n",
    "def traverse_ability_tree(node):\n",
    "    global index\n",
    "    ability_index_dict[node[\"name\"]] = index\n",
    "    index+=1\n",
    "    if \"children\" in node:\n",
    "        for child in node[\"children\"]:\n",
    "            traverse_ability_tree(child)\n",
    "\n",
    "traverse_ability_tree(ability_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "#read in tagged data\n",
    "instance_dataset_path = \"src/instances_Inception_600_with_tag.csv\"\n",
    "df = pd.read_csv(instance_dataset_path)\n",
    "instance_cnt = len(df)\n",
    "print(instance_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# init ability data\n",
    "if os.path.exists(\"statistics/ability_pair_ref_600.pickle\"):\n",
    "    with open(\"statistics/ability_pair_ref_600.pickle\",'rb') as f:\n",
    "        ability_pair_ref = pickle.load(f)\n",
    "else:\n",
    "    # first calculate the tagged ability appearances in raw df\n",
    "    ability_freq = {} \n",
    "    for i in range(len(df)):\n",
    "        abilities = df.iloc[i][\"annotation\"]\n",
    "        ability_list = abilities.split(\",\")\n",
    "        for ability in ability_list:\n",
    "            if ability not in ability_freq:\n",
    "                ability_freq[ability] = 0\n",
    "            ability_freq[ability] += 1\n",
    "        \n",
    "    def add_freq_to_parent(node):\n",
    "        if \"children\" in node:\n",
    "            for child in node[\"children\"]:\n",
    "                add_freq_to_parent(child)\n",
    "                ability_freq[node[\"name\"]] = ability_freq.get(node[\"name\"],0) + ability_freq.get(child[\"name\"],0)\n",
    "        else:\n",
    "            ability_freq[node[\"name\"]] = ability_freq.get(node[\"name\"],0)\n",
    "    \n",
    "    add_freq_to_parent(ability_tree)\n",
    "    # normalize the ability freq\n",
    "    total = len(df)\n",
    "    for ability in ability_freq:\n",
    "        ability_freq[ability] /= total\n",
    "\n",
    "    # then calculate the ability pair frequency\n",
    "    ability_pair_ref = np.zeros((len(ability_index_dict),len(ability_index_dict)),dtype=np.float32)\n",
    "    for ability,index in ability_index_dict.items():\n",
    "        for ability_2,index_2 in ability_index_dict.items():\n",
    "            ability_pair_ref[index][index_2] = ability_freq[ability] * ability_freq[ability_2]\n",
    "    with open(\"statistics/ability_pair_ref_600.pickle\",'wb') as f:\n",
    "        pickle.dump(ability_pair_ref,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "#read in tagged data\n",
    "instance_dataset_path = \"src/instances_Inception_150_with_tag.csv\"\n",
    "df = pd.read_csv(instance_dataset_path)\n",
    "instance_cnt = len(df)\n",
    "print(instance_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# init ability data\n",
    "if os.path.exists(\"statistics/ability_pair_ref_150.pickle\"):\n",
    "    with open(\"statistics/ability_pair_ref_150.pickle\",'rb') as f:\n",
    "        ability_pair_ref = pickle.load(f)\n",
    "else:\n",
    "    # first calculate the tagged ability appearances in raw df\n",
    "    ability_freq = {} \n",
    "    for i in range(len(df)):\n",
    "        abilities = df.iloc[i][\"annotation\"]\n",
    "        ability_list = abilities.split(\",\")\n",
    "        for ability in ability_list:\n",
    "            if ability not in ability_freq:\n",
    "                ability_freq[ability] = 0\n",
    "            ability_freq[ability] += 1\n",
    "        \n",
    "    def add_freq_to_parent(node):\n",
    "        if \"children\" in node:\n",
    "            for child in node[\"children\"]:\n",
    "                add_freq_to_parent(child)\n",
    "                ability_freq[node[\"name\"]] = ability_freq.get(node[\"name\"],0) + ability_freq.get(child[\"name\"],0)\n",
    "        else:\n",
    "            ability_freq[node[\"name\"]] = ability_freq.get(node[\"name\"],0)\n",
    "    \n",
    "    add_freq_to_parent(ability_tree)\n",
    "    # normalize the ability freq\n",
    "    total = len(df)\n",
    "    for ability in ability_freq:\n",
    "        ability_freq[ability] /= total\n",
    "\n",
    "    # then calculate the ability pair frequency\n",
    "    ability_pair_ref = np.zeros((len(ability_index_dict),len(ability_index_dict)),dtype=np.float32)\n",
    "    for ability,index in ability_index_dict.items():\n",
    "        for ability_2,index_2 in ability_index_dict.items():\n",
    "            ability_pair_ref[index][index_2] = ability_freq[ability] * ability_freq[ability_2]\n",
    "    with open(\"statistics/ability_pair_ref_150.pickle\",'wb') as f:\n",
    "        pickle.dump(ability_pair_ref,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 32)\n",
      "333    你是股份回购领域的专家，请你用通俗、简洁的语言向一位初学者介绍事件验证法的概念与要点\n",
      "132                                     请在15分钟后叫我\n",
      "155                           请帮我做一个个人简历，要能体现我的牛逼\n",
      "23                               帮我在室友群问一下他们晚上的安排\n",
      "211                              请帮我整理与大语言模型相关的文献\n",
      "186                             帮我根据已有信息整理出一篇实验报告\n",
      "183                                 让我一个小时之内不要碰手机\n",
      "78                         这是一组数据，请问你观察到了什么有价值的信息\n",
      "128                        【在材料页面】阅读我写好的辩诉材料，帮我修改\n",
      "345                  【在线上参会】我去上厕所，帮我记一下，有人叫我就震动手环\n",
      "Name: expression, dtype: object [0.00181436 0.00181437 0.0018146  0.00181936 0.00182418 0.00182695\n",
      " 0.00182806 0.00183602 0.00185339 0.00186943]\n"
     ]
    }
   ],
   "source": [
    "#generatre a frequency table for the data\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "instances_with_embedding_path = \"src/instances_Inception_600_with_similarity.csv\"\n",
    "\n",
    "df_embedding = pd.read_csv(instances_with_embedding_path)\n",
    "\n",
    "# read from the embedding_pca column and convert it to numpy array, the split symbol is space like \"[1.5 2.8 ... 3.97]\"\n",
    "pca_array = np.array([np.fromstring(embedding[1:-1],dtype=float,sep=' ') for embedding in df_embedding.embedding_pca])\n",
    "\n",
    "print(pca_array.shape)\n",
    "\n",
    "# use kmeans and set k=1 to get the center\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(pca_array)\n",
    "\n",
    "# calculate the distance between each of the 4 centers and each element in pca_array and take the distance of the closest center as the distance to the center\n",
    "distance = np.array([min([np.linalg.norm(center - embedding) for center in kmeans.cluster_centers_]) for embedding in pca_array])\n",
    "\n",
    "# calculate the probability of each example using p(i) = exp(-d(i)^2/2sigma^2)\n",
    "sigma = 0.5\n",
    "probability = np.exp(-distance**2/(2*sigma**2))\n",
    "\n",
    "# normalize the probability\n",
    "probability = probability / probability.sum()\n",
    "\n",
    "#output the top ten probability's corresponding instance expression\n",
    "top_ten = np.argsort(probability)[-10:]\n",
    "print(df_embedding.iloc[top_ten].expression, probability[top_ten])\n",
    "\n",
    "\n",
    "# output the probability into pickle\n",
    "with open(\"statistics/instance_prob_600.pkl\", \"wb\") as f:\n",
    "    pickle.dump(probability, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 32)\n",
      "70                    请帮我记录一个明天早上7点的提醒事项\n",
      "73                   帮我写篇拓扑几何前沿研究主题的文献综述\n",
      "119    请在接下来2小时内帮我自动回复导师的信息，帮我掩饰我在外面玩的事情\n",
      "60                           规划一下明天的行程安排\n",
      "110                   请帮我写一篇能发表在cvpr上的论文\n",
      "109           如果我需要做一份关于社会实践的报告，请给出报告的大纲\n",
      "133                     提醒我明天七点到清华大学（导航）\n",
      "126                        提前10分钟提醒我参加组会\n",
      "118                          根据我的关键词查找文献\n",
      "1                        请帮我找出与这篇文章相似的例子\n",
      "Name: expression, dtype: object [0.00724914 0.00724965 0.00725133 0.00731024 0.00735349 0.00741306\n",
      " 0.00742145 0.00745267 0.00747263 0.00753554]\n"
     ]
    }
   ],
   "source": [
    "#generatre a frequency table for the data\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "instances_with_embedding_path = \"src/instances_Inception_150_with_similarity.csv\"\n",
    "\n",
    "df_embedding = pd.read_csv(instances_with_embedding_path)\n",
    "\n",
    "# read from the embedding_pca column and convert it to numpy array, the split symbol is space like \"[1.5 2.8 ... 3.97]\"\n",
    "pca_array = np.array([np.fromstring(embedding[1:-1],dtype=float,sep=' ') for embedding in df_embedding.embedding_pca])\n",
    "\n",
    "print(pca_array.shape)\n",
    "\n",
    "# use kmeans and set k=1 to get the center\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(pca_array)\n",
    "\n",
    "# calculate the distance between each of the 4 centers and each element in pca_array and take the distance of the closest center as the distance to the center\n",
    "distance = np.array([min([np.linalg.norm(center - embedding) for center in kmeans.cluster_centers_]) for embedding in pca_array])\n",
    "\n",
    "# calculate the probability of each example using p(i) = exp(-d(i)^2/2sigma^2)\n",
    "sigma = 0.5\n",
    "probability = np.exp(-distance**2/(2*sigma**2))\n",
    "\n",
    "# normalize the probability\n",
    "probability = probability / probability.sum()\n",
    "\n",
    "#output the top ten probability's corresponding instance expression\n",
    "top_ten = np.argsort(probability)[-10:]\n",
    "print(df_embedding.iloc[top_ten].expression, probability[top_ten])\n",
    "\n",
    "\n",
    "# output the probability into pickle\n",
    "with open(\"statistics/instance_prob_150.pkl\", \"wb\") as f:\n",
    "    pickle.dump(probability, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
